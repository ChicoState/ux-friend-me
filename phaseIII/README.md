# Phase III: Usability Evaluation

## Introduction

Phase III was conducted in a culmination of the several phases of the project, namely we took what analyses we had done and visual concepts we had drawn in order to create an interactive prototype of the FriendMe application. This prototype was then tested with users in order to evaluate the usability of the interface design and the overall user experience. Through this high-fidelity experience, we were able to get close to the final product in order to get the most accurate feedback from users on the design and functionality for future improvements.

## Methods

To start, we developed a prototype of our application that was interactive and similar in functionality to what we envisioned the final product to be. This prototype was developed using Figma, a web-based design tool that allows for the creation of interactive prototypes. With the prototype complete, we were able to conduct what is called a usability test, which is a method of evaluating a product by testing it on users. This method allows for the identification of any usability problems, understanding the user's satisfaction with the product, and determining the user's ability to complete tasks. We reserved private rooms at the university library where we had one researcher and one participant in each room. The researcher would guide the participant through a series of tasks while the participant would think out loud as they completed the tasks.

To aid in the usability test, a key deliverable in Phase III involved the development of a usability protocol that would help guide the evaluation of the FriendMe application. This protocol had a preliminary introduction to the usability test and the application, a few background questions to get to know the user's experience with scheduling apps, a series of tasks that the user would have to complete while using the prototype, and a series of post-test questions to punctuate the session.

Our introduction to the usability test higlighted the purpose of the test, the importance of the user's feedback, and the confidentiality of the test. Ethical considerations are important in usability testing, and so we wanted to make sure that the user felt comfortable and understood the purpose of the test. We also wanted to make sure that the user understood that the session was not a test of their abilities, but rather an effort to improve the application itself. All researchers were certified by the CITI "Social and Behavioral Responsible Conduct of Research" course, which helped us understand the ethical considerations of conducting research with human subjects.

The questions that followed were meant to get a sense of the user's background with scheduling applications:

- Describe how you would go about planning to socialize with one or a group of your friends currently.
- When you are trying to connect with friends, what app are you most likely to use?
- Does this app facilitate scheduling? If so, how?
- When someone wants to ‘add you’ on an app, what piece of information is the first thing you expect them to ask?
- In your experience, what drives you to click on important information on an app?
- What is one feature you would expect in a social scheduling app?

The main motivation behind these line of questions was to get a sense of what the user's expectations were for a scheduling application, and to see if they had any prior experience with similar concepts. This would help us understand the user's perspective and how they might interact with the FriendMe application, perhaps giving us some insight into the difficulty rating of the tasks we would ask them to complete.

The tasks that the user had to complete were as follows:

1. Create a new event and invite three friends.
2. Search for a local event by typing in a keyword into the search bar.
3. Delete an event.

Each task was actually phrased as a scenario to leave room for as much creativity as possible. As stated this was a 'think-aloud' approach, which just means that the user was encouraged to communicate their thought process out loud so that the researchers could understand their reasoning and decision-making process. After every task, the user was asked to rate the difficulty of the task on a scale of 1 to 5, with 1 being very easy and 5 being very difficult. We wanted to make sure to have a quantitative measure of the user's experience with the prototype to better inform every qualitative response we received. Each task was also rated as a success or failure based on the user's ability to complete the task, but this result was not shared with the user.

Finally, post-test questions were asked to get a sense of the user's overall experience with the prototype and how the study was conducted. These questions were as follows:

1. What did you like about this study?
2. What didn't you like?
3.  What were your thoughts on the calendar tool as the main focal point of user interaction?
4. Was the navigation of the app easy to follow?

It was important to the team that every aspect of this test served as an opportunity for improvement, and that includes the conduct of the study itself. Questions 1 and 2 were meant to address this. Question 3 was to get the user's opinion on one of the main features of the scheduling tool, the calendar tool. It was a difficult interface to design for the team that went through several phases of design, so any feedback was important. Finally, question 4 was just a general question to get a sense of the user's experience with the prototype's navigation.

## Findings

For the usability test, we anonymized the data and compiled it into a spreadsheet for analysis. For ethical considerations, any physical notes taken during the test were destroyed after the data was compiled. We used a stacked percentage bar graph to visualize the quantitative results, or rather the difficulty ratings for each task. The results are as follows:

![Graph 1](./images/graph1.png)

As for the success rates of each task, we found that Task 1 had a 100% success rate, Task 2 had a 90% success rate, and Task 3 had a 71% success rate. This data represented our qualitative results.

For detailed results, please refer to the [FriendMe: User Test Data Collection](https://docs.google.com/spreadsheets/d/1BvM-nbqoXXSm17ktTU95gF02FkI2WW9b6tgC1M8cS-w/edit?usp=sharing) spreadsheet mentioned previously.

## Conclusions

The usability testing phase for the FriendMe application provided invaluable insights into user interactions and satisfaction with the prototype. Overall, participants found the interface to be intuitive and the tasks reasonably straightforward, as evidenced by the generally low difficulty ratings across tasks. From the figure above, we see that more than 70% of ratings given for Tasks 1 and 2 were 'very easy' or 'easy'. Task 3 skewed a little higher, but the trend still showed that 86% of users rated the task at most 'moderate', or a 3 out of 5.

However, specific areas of improvement were identified that could enhance the user experience significantly. For context, Task 3 had users delete a previously-scheduled event using the FriendMe prototype. As can be gleaned from our data, Task 3 was the most difficult to complete and we might attribute that due to lack of visibility of previously-scheduled events in the user profile. Where most other options were clearly-bolded or had pictures to attribute their function, this particular task was well-hidden and lacked a clear call-to-action.

Although the rest of the tasks proved mostly successful, we might also improve the usability function for Task 2, which had users search for a local event by typing in a keyword. The only way to solve this problem in the test was by typing into the search bar, but it would have been a great addition to also include the solution as an event listed within the 'filtered events' section of our prototype (which was also the main page). Most users, we noticed, looked there first.

Overall, navigation within the app received positive remarks for its simplicity, though suggestions for more direct shortcuts to frequent actions could reduce the number of steps users need to undertake.

## Caveats

There were a few caveats that may have influenced the outcome of our study. Namely, the population of participants was comprised of only classmates enrolled in the CSCI 431W Usability Engineering course most of whom were within a similar age group. Because our application is meant for a wide range of users, it would have been beneficial to have a more diverse group of participants. Additionally, the usability tests across several projects were performed consecutively, which may have led to some fatigue in the participants. 


